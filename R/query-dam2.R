#' @importFrom data.table ":="
#' @importFrom data.table "key"
#' @import behavr
NULL
#' Retrieves DAM2 data from one or several continuous files
#'
#' Uses a query mechanism to get data from a DAM2 array.
#' This is useful when using the default behaviour of TriKinetics software
#' where data is simply appended to a single long file per monitor.
#'
#' @param result_dir the root directory where all daily data are saved
#' @param query [data.frame] representing a formatted query used to request data (see detail)
#' @param FUN function (optional) to transform the data from each animal
#' immediately after is has been loaded.
#' @param ... extra arguments to be passed to `FUN`
#' @return A [behavr] table.
#' The metadata contains all the query columns and an autogenerated id per animal.
#' The data has the columns:
#' * `id` -- autogenerated unique identifier, one per animal
#' *  `t` -- time
#' * `activity` -- number of beam crosses
#' @details `query` must be a [data.frame] (or [data.table]) object.
#' Conceptually, each row of the query describes one animal with one set of conditions (when `region_id` is specified),
#' or in each monitor (when it is not).
#' It must have  the following columns:
#' * `file` -- the location of a data file (e.g. `"Monitor3.txt"`).
#' * `start_datetme` -- the first day and time of the requested experiment (e.g. `"2014-12-28 18:00:00"`).
#' * `stop_datetime` -- the last day and time of the requested experiment (e.g. `"2014-12-30  19:00:00"` or simply "2014-12-30").
#' * `region_id` -- the channel (between 1 and 32) in which the animal was in (e.g. `20`).
#'   `region_id` is optional. If not provided, all 32 channels are loaded *with the same conditions*.
#' * `???` *any number of arbitrary columns* to associate `conditions`/`treatments`/`genotypes`/... to the previous columns.
#'
#' The time in data is expressed relatively to start_date.
#' In other words, if you do circadian analysis, and your D-L transitions are at 09:00:00, you want to set
#' `start_datetime = "YYY-MM-DD 09:00:00"`. The root directory is the folder where your files live.
#' For instance, `result_dir = "C:/where/I/Store/my/txt/files/"`
#'
#' @examples
#' # This is where our data lives
#' root_dir <- damr_example_dir()
#'
#' # A query already made for us.
#' # It defines condition and genotype of each animal
#' data(single_file_query)
#' print(single_file_query)
#'
#' # we find and load the matching data
#' dt <- query_dam2(root_dir,single_file_query)
#' print(dt)
#'
#' # genotype and condition to our metadata:
#' print(dt[meta=TRUE])
#'
#' # Just the first few reads, we run `head()` on each animal
#' dt <- query_dam2(root_dir, single_file_query, FUN=head)
#' print(dt)
#' @seealso [read_dam2_file] to to load data from a single file (without a query).
#' @export
query_dam2 <- function(result_dir, query, FUN=NULL, ...){
  tz="UTC"

  check_dir_exists(result_dir)
  q <- data.table::copy(data.table::as.data.table(query))

  cn <- colnames(q)
  if(!("file" %in% cn & "start_datetime" %in% cn & "stop_datetime" %in% cn )){
    stop("query MUST have at least three columns names `file`, `start_datetime` and `stop_datetime`")
  }

  q[,path:=paste(result_dir,file,sep="/")]
  q[,file:=NULL]

  # force format for midnight dates (#6)
  q[, start_datetime := sapply(start_datetime, function(x) format(parse_datetime(x), format = "%F %T"))]

  q[,experiment_id:=sprintf("%s|%s",
                start_datetime,
                 basename(path))]

  # we expand query for all channels if no channel provided
  if(!"region_id" %in% cn)
    q <- q[q[,.(region_id=1:32),by=experiment_id], on="experiment_id"]

  q[,id:=sprintf("%02d|%s",
                 region_id,
                 experiment_id)]
  # TODO check for uniqueness in query!!
  to_read <- q[,.(regions = list(region_id)),by=c("path","start_datetime","stop_datetime")]
  s <- to_read[,
               list(data=list(read_dam2_file(path,
                                                    regions[[1]],
                                                    start_datetime,
                                                    stop_datetime,
                                                    tz=tz)
                              )
                    ),
               by="path,start_datetime,stop_datetime"]

  d <- behavr::bind_behavr_list(s[,data])

  # new metadata had the columns in q that are not in metadata already

  met <- d[meta=T]
  met <- met[
    q[,
      c("id", setdiff(colnames(q[,-"path"]), colnames(met))),
      with=F],
    on="id"]

  data.table::setkeyv(met,"id")
  # replace metadata

  behavr::setmeta(d, met)
  if(!is.null(FUN))
    d <- d[,FUN(.SD, ...),by="id"]
  d
}


